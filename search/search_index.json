{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#zarp","title":"ZARP","text":"<p>Welcome to the ZARP documentation pages!</p> <p>ZARP is a generic RNA-Seq analysis workflow that allows users to process and analyze Illumina short-read sequencing libraries with minimum effort. Better yet: With our companion ZARP-cli command line interface, you can start ZARP runs with the simplest and most intuitive commands.</p> <p>RNA-seq analysis doesn't get simpler than that!</p> <p>The workflow is developed in Snakemake, a widely used workflow management system in the bioinformatics community. ZARP will pre-process, align and quantify your single- or paired-end stranded bulk RNA-seq sequencing libraries with publicly available state-of-the-art bioinformatics tools. ZARP's browser-based rich reports and visualizations will give you meaningful initial insights in the quality and composition of your sequencing experiments - fast and simple. Whether you are an experimentalist struggling with large scale data analysis or an experienced bioinformatician, when there's RNA-seq data to analyze, just zarp 'em!</p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>ZARP requires Conda to install the basic dependencies. Each individual step of the workflow runs either in its own Apptainer container or in its own Conda virtual environment.</p> <p>Once the installation is complete, you fill in a <code>config.yaml</code> file with parameters and a <code>samples.tsv</code> file with sample-specific information. You can easily trigger ZARP by making a call to Snakemake with the appropriate parameters.</p> <p>ZARP can be executed in different systems or High Performance Computing (HPC) clusters. ZARP generates multiple output files that help you quality control (QC) your data and proceed with downstream analyses. Apart from running the main ZARP workflow, we also provide you with two auxiliary workflows for fetching sequencing libraries from the Sequence Read Archive (SRA) and for inferring missing sample metadata straight from the data itself (both of which are integrated in ZARP-cli for your convenience).</p>"},{"location":"#how-to-cite","title":"How to cite","text":"<p>If you use ZARP in your work, please kindly cite the following article:</p> <p>ZARP: A user-friendly and versatile RNA-seq analysis workflow Maria Katsantoni, Foivos Gypas, Christina J. Herrmann, Dominik Burri, Maciej Bak, Paula Iborra, Krish Agarwal, Meric Ataman, M\u00e1t\u00e9 Balajti, No\u00e8 Pozzan, Niels Schlusser, Youngbin Moon, Aleksei Mironov, Anastasiya B\u00f6rsch, Mihaela Zavolan, Alexander Kanitz F1000Research 2024, 13:533 https://doi.org/10.12688/f1000research.149237.1</p>"},{"location":"#info-materials","title":"Info materials","text":""},{"location":"#poster","title":"Poster","text":""},{"location":"#reach-out","title":"Reach out","text":"<p>There are several ways to get in touch with us:</p> <ul> <li>For ZARP usage questions, please use the ZARP Q&amp;A forum   (requires GitHub registration).</li> <li>For feature suggestions and bug reports, please use either the   ZARP or ZARP-cli issue   tracker (requires GitHub registration).</li> <li>For any other requests, please reach out to us via email.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We always welcome and duly acknowledge open source contributors, for ZARP, ZARP-cli or any other of our projects. Simply follow our onboarding instructions and please mind our Code of Conduct. If you have any questions, do not hesitate to shoot us us an email.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":""},{"location":"guides/installation/","title":"Installation","text":"<p>On this page, you will find out how to install ZARP on your system.</p>"},{"location":"guides/installation/#requirements","title":"Requirements","text":"<p>For improved reproducibility and reusability, each individual workflow step runs either inside a dedicated Conda environment or an (Apptainer) container. As a consequence, running ZARP only has very few dependencies, that need to be available on your system:</p> <ul> <li>Linux (tested with <code>Ubuntu 24.04</code>)</li> <li>Conda (tested with <code>Conda 24.11.3</code>)</li> <li>Optional: Apptainer (tested with <code>Apptainer 1.3.6</code>)</li> </ul> <p>A few additional dependencies are installed via Conda as described further below.</p> <p>Other versions, especially older ones, are not guaranteed to work!</p> Don't have Linux? <p>Please see the \"How to use Docker?\" instructions in the usage section.</p> How do I install Apptainer? <p>Please follow the official documentation to install Apptainer (formerly Singularity) globally and configure its permissions.</p>"},{"location":"guides/installation/#installation-steps","title":"Installation steps","text":""},{"location":"guides/installation/#1-clone-zarp","title":"1. Clone ZARP","text":"<p>Clone the ZARP workflow repository with:</p> <pre><code>git clone https://github.com/zavolanlab/zarp.git\n</code></pre>"},{"location":"guides/installation/#2-ensure-conda-is-available","title":"2. Ensure Conda is available","text":"<p>To check if you already have Conda installed on your system, type:</p> <pre><code>conda --version\n</code></pre> <p>If Conda is available, you should see output similar to this:</p> <p><pre><code>conda 24.11.3\n</code></pre> If it is not installed, you will instead see <code>command not found: conda</code>.</p> What's the best way to install Conda? <p>Conda can be installed in multiple ways. We strongly recommend using the Miniforge distribution, as it is built around the community-supplied <code>conda-forge</code> channel that is heavily used by ZARP. It also reduces the risk of accidentally violating Conda's licensing restrictions by using the <code>defaults</code> channel.</p> <p>Please refer to the official documentation for up-to-date installation instructions.</p> My Conda version is not compatible <p>After completing Conda setup, you can install a specific Conda version with the following command:</p> <pre><code>conda install conda=24.11.3\n</code></pre> I do not want to change my Conda version <p>If you already have a specific Conda version on your system that is not compatible with ZARP, and you do not want to change it, no worries. Just indicate a different directory when using the interactive Miniforge installer. Then source the appropriate <code>conda.sh</code> file to switch between versions, e.g.:</p> <pre><code>source $HOME/miniconda3/etc/profile.d/conda.sh  # OR\nsource $HOME/miniconda3_alt/etc/profile.d/conda.sh\n</code></pre>"},{"location":"guides/installation/#3-set-up-your-zarp-environment","title":"3. Set up your ZARP environment","text":"<p>To install the remaining ZARP dependencies, run:</p> <pre><code>conda env create -f install/environment.yml\n</code></pre> Installing development dependencies <p>If you would like to run tests or if you are planning to contribute to ZARP's development, run the following command instead to set up your ZARP Conda environment.</p> <pre><code>conda env create -f install/environment.dev.yml\n</code></pre> <p>This will ensure that all development dependencies are installed as well.</p> You want to run ZARP on an HPC? <p>When running ZARP on a High-Performance Computing cluster, you will need to make sure that compatible versions of at least one of Conda (when using Snakemake's <code>--use-conda</code> option) and Apptainer (when using the <code>--use-apptainer</code> option) are installed and properly configured on each machine of the cluster, including the head node. Reach out to your systems administrator to set this up for you.</p>"},{"location":"guides/installation/#4-activate-the-zarp-environment","title":"4. Activate the ZARP environment","text":"<p>Activate the Conda environment with:</p> <pre><code>conda activate zarp\n</code></pre>"},{"location":"guides/installation/#running-installation-tests","title":"Running installation tests","text":"<p>We have prepared several tests to check the integrity of the workflow and its components. These can be found in subdirectories of the <code>tests/</code> directory. The most critical of these tests enable you to execute the entire workflow on a set of small example input files. Note that for this and other tests to complete successfully, additional development dependencies need to be installed.</p> <p>Execute the commands below to run the test workflow on your local machine or on your Slurm-managed HPC cluster.</p> Failing tests do not necessarily indicate a problem! <p>Our tests were developed to guard against code regression over time or as a result of proposed changes and are therefore very rigorous. In particular, even the minutest changes in outputs, even individual pixels or metadata in the produced output images, will cause a test suite to fail. However, we cannot rule out such minor changes across systems, and therefore, it is quite possible that your test runs may fail, even though the workflow is properly installed and functional. Check the logs to see whether Snakemake completed successfully. If so, it is very likely that everything is fine.</p>"},{"location":"guides/installation/#running-tests-on-your-local-machine","title":"Running tests on your local machine","text":"<p>Use the following command to run each step inside a dedicated Conda environment:</p> <pre><code>bash tests/test_integration_workflow_with_conda/test.local.sh\n</code></pre> <p>Instead, use the following command to run each step inside an Apptainer container:</p> <pre><code>bash tests/test_integration_workflow_with_conda/test.slurm.sh\n</code></pre>"},{"location":"guides/installation/#running-tests-on-your-slurm-cluster","title":"Running tests on your Slurm cluster","text":"<p>Use the following command to run each step inside a dedicated Conda environment:</p> <pre><code>bash tests/test_integration_workflow/test.local.sh\n</code></pre> <p>Instead, use the following command to run each step inside an Apptainer container:</p> <pre><code>bash tests/test_integration_workflow/test.slurm.sh\n</code></pre> The Slurm tests are failing for me! <p>Depending on the configuration of your Slurm installation you may need to adapt file <code>profiles/slurm-config.json</code> and the arguments to options <code>--cores</code> and <code>--jobs</code> in the file <code>config.yaml</code> of a respective profile. Consult the manual of your workload manager as well as the section of the Snakemake manual dealing with profiles.</p>"},{"location":"guides/outputs/","title":"Output files","text":"<p>Here you can find an overview of the output files for the different workflows.</p>"},{"location":"guides/outputs/#outputs-of-zarp","title":"Outputs of ZARP","text":"<p>After running the ZARP workflow, you will find several output files in the  specified output directory. The output directory is defined in the  <code>config.yaml</code> file and it is normally called <code>results/</code>. Here are some of the  key output files:</p> <ul> <li> <p>Quality control: ZARP generates comprehensive quality control reports    that provide insights into the quality and composition of the sequencing    experiments. These reports include metrics such as read quality, alignment    statistics, and gene expression summaries.</p> </li> <li> <p>Quantification files: These files contain the gene and transcript level    expression values for each sample. They provide information about the    abundance of each gene and transcript in the RNA-seq data.</p> </li> <li> <p>Alignment files: These files contain the aligned reads for each sample   in BAM format. They provide information about the mapping of the reads to   the reference genome.</p> </li> </ul> <p>After a run you will, you will find all outputs within the <code>results/</code> directory, arranged in the following general structure:</p> <pre><code>.\n\u2514\u2500\u2500 mus_musculus\n    \u251c\u2500\u2500 multiqc_summary\n    \u251c\u2500\u2500 samples\n    \u251c\u2500\u2500 summary_kallisto\n    \u251c\u2500\u2500 summary_salmon\n    \u2514\u2500\u2500 zpca\n</code></pre> <p>Here is a description of the different subdirectories:</p> <ul> <li><code>mus_musculus/</code>: A subdirectory for organism-specific results.</li> <li><code>multiqc_summary/</code>: Summary files generated by MultiQC.</li> <li><code>samples/</code>: Sample-specific outputs. One directory is created for     each sample.</li> <li><code>summary_kallisto/</code>: Summary files for Kallisto quantifications.</li> <li><code>summary_salmon/</code>: Summary files for Salmon quantifications.</li> <li><code>zpca/</code>: Output files for ZARP's principal component analysis.</li> </ul>"},{"location":"guides/outputs/#quality-control-outputs","title":"Quality Control outputs","text":"<p>Within the <code>multiqc_summary/</code> directory, you will find an interactive HTML file <code>multiqc_report.html</code> with various quality control (QC) metrics that can help you interpret your results. An example file is shown below:</p> <p>On the left you can find a navigation bar that directs you to different sections and subsections for various tools:</p> <ul> <li>The <code>General Statistics</code> section contains a summary of most tools and you can   find statistics on mapped reads, percent of duplicate reads, percent of   adapters trimmed for various tools:</li> </ul> <ul> <li>The <code>FastQC: raw reads</code> section contains plots and quality statistics of your   input FASTQ files. Some examples are shown below, like the number of   duplicate reads in an experiment, the average sequencing quality per   position, or the percentage of GC content:</li> </ul> <ul> <li>The <code>Cutadapt: adapter removal</code> and <code>Cutadapt: polyA tails removal</code> shows the   number or the percentage of the reads trimmed:</li> </ul> <ul> <li> <p>The <code>FastQC: trimmed reads</code> section contains plots and quality statistics of   the FASTQ files after adapter trimming. The plots are similar to the section   <code>FastQC: raw reads</code>.</p> </li> <li> <p>The <code>STAR</code> section shows the number and percentage of reads that are mapped   using the STAR aligner:</p> </li> </ul> <ul> <li>The <code>ALFA</code> section shows the number of reads mapped to genomic categories   (stop codon, 5'-UTR, CDS, intergenic, etc.) and gene biotypes (protein coding   genes, miRNA , tRNA, etc.) for unique reads and multimappers:</li> </ul> <ul> <li>The <code>TIN</code> section shows the Transcript Integrity Number of the samples:</li> </ul> <ul> <li>The <code>Salmon</code> section shows the fragment length distribution of the reads:</li> </ul> <ul> <li>The <code>Kallisto</code> section shows the number of reads that were aligned:</li> </ul> <ul> <li>Finally the <code>zpca</code> Salmon and Kallisto sections show PCA plots for expression   levels of genes and transcripts:</li> </ul>"},{"location":"guides/outputs/#quantification-outputs","title":"Quantification outputs","text":"<p>Within the <code>summary_kallisto</code> directory, you can find the following files:</p> <ul> <li><code>genes_counts.tsv</code>: A table with gene counts. The first column (index)   contains the gene names and the first row (column) contains the sample   names. This file can later be used for downstream differential expression   analysis.</li> <li><code>genes_tpm.tsv</code>: A table with gene TPM estimates.</li> <li><code>transcripts_counts.tsv</code>: A table with transcript counts. The first column   (index) contains the transcript names and the first row (column) contains   the sample names. This file can later be used for downstream differential   transcript analysis.</li> <li><code>transcripts_tpm.tsv</code>: A table with the transcript TPM estimates.</li> <li><code>tx2geneID.tsv</code>: A table mapping transcript IDs to gene IDs.</li> </ul> <p>Within the <code>summary_salmon/quantmerge</code> directory, you can find the following files:</p> <ul> <li><code>genes_numreads.tsv</code>: A table with gene counts. The first column (index)   contains the gene names and the first row (column) contains the sample   names. This file can later be used for downstream differential expression   analysis.</li> <li><code>genes_tpm.tsv</code>: Matrix with the gene TPM estimates.</li> <li><code>transcripts_numreads.tsv</code>: Matrix with the transcript counts. The first   column (index) contains the transcript names and the first row (column)   contains the sample names. This file can later be used for downstream   differential transcript analysis.</li> <li><code>transcripts_tpm.tsv</code>: Matrix with the transcript TPM estimates.</li> </ul>"},{"location":"guides/outputs/#alignment-outputs","title":"Alignment outputs","text":"<p>Within the <code>samples</code> directory, you can find a directory for each sample, and within these directories you can find the output files of the individual steps. Some alignment files can be easily used to open in a genome browser for other downstream analysis:</p> <ul> <li>In the <code>map_genome</code> directory you can find a file with the suffix   <code>.Aligned.sortedByCoord.out.bam</code> and the corresponding indexed (<code>.bai</code>)   file. This is the output of the STAR aligner.</li> <li>In the <code>bigWig</code> directory you can find two folders. <code>UniqueMappers</code> and   <code>MultimappersIncluded</code>. Within these files you find the bigWig files for the   plus and minus strand. These files are convenient to load in a genome   browser (like IGV) to view the genome coverage of the mappings.</li> </ul>"},{"location":"guides/outputs/#outputs-of-the-sra-download-workflow","title":"Outputs of the SRA download workflow","text":"<p>Once you run the workflow that downloads data from the Sequence Read Archive (SRA), you can find the following file structure:</p> <pre><code>results/\n`-- sra_downloads\n    |-- compress\n    |   |-- ERR2248142\n    |   |   |-- ERR2248142.fastq.gz\n    |   |   `-- ERR2248142.se.tsv\n    |   |-- SRR18549672\n    |   |   |-- SRR18549672.pe.tsv\n    |   |   |-- SRR18549672_1.fastq.gz\n    |   |   `-- SRR18549672_2.fastq.gz\n    |   `-- SRR18552868\n    |       |-- SRR18552868.fastq.gz\n    |       `-- SRR18552868.se.tsv\n    |-- fasterq_dump\n    |   `-- tmpdir\n    |-- get_layout\n    |   |-- ERR2248142\n    |   |   `-- SINGLE.info\n    |   |-- SRR18549672\n    |   |   `-- PAIRED.info\n    |   `-- SRR18552868\n    |       `-- SINGLE.info\n    |-- prefetch\n    |   |-- ERR2248142\n    |   |   `-- ERR2248142.sra\n    |   |-- SRR18549672\n    |   |   `-- SRR18549672.sra\n    |   `-- SRR18552868\n    |       `-- SRR18552868.sra\n    `-- sra_samples.out.tsv\n</code></pre> <p>All results are stored under the output directory you have specified in your <code>config.yaml</code> file (<code>results/</code> in this case). The <code>sra_samples.out.tsv</code> summarizes all the experiments that were fetched from SRA. The file contains the SRR experiment and the path to FASTQ file(s). An example output file looks like the following:</p> <pre><code>sample  fq1     fq2\nSRR18552868     results/sra_downloads/compress/SRR18552868/SRR18552868.fastq.gz \nSRR18549672     results/sra_downloads/compress/SRR18549672/SRR18549672_1.fastq.gz       results/sra_downloads/compress/SRR18549672/SRR18549672_2.fastq.gz\nERR2248142      results/sra_downloads/compress/ERR2248142/ERR2248142.fastq.gz \n</code></pre> <p>Single vs. paired-end sequencing</p> <p>Some of the filenames indicate if the sample was sequenced in <code>SINGLE (se)</code>- or <code>PAIRED (pe)</code>-end mode.</p>"},{"location":"guides/outputs/#outputs-of-the-metadata-inference-workflow","title":"Outputs of the metadata inference workflow","text":"<p>Once you run the workflow that infers metadata you can find the following file structure:</p> <pre><code>results/\n|-- FVKEQ\n|   |-- library_source_testpath1.1.fastq.json\n|   |-- library_source_testpath1.2.fastq.json\n|   |-- read_layout_testpath1.1.fastq.json\n|   `-- read_layout_testpath1.2.fastq.json\n|-- HGLR5\n|   |-- library_source_testpath2.1.fastq.json\n|   `-- read_layout_testpath2.1.fastq.json\n|-- htsinfer_SRR1.json\n|-- htsinfer_SRR2.json\n`-- samples_htsinfer.tsv\n</code></pre> <p>All results are stored under the output directory you have specified in your <code>config.yaml</code> file (<code>results/</code> in this case). A JSON file with the <code>htsinfer_</code> prefix is generated containing the inferred metadata for each of the samples. All information that could be determined are stored in the file <code>samples_htsinfer.tsv</code>, which can later be used in the main ZARP workflow.</p>"},{"location":"guides/parameterization/","title":"Parameterization","text":""},{"location":"guides/parameterization/#adjusting-tool-parameters","title":"Adjusting tool parameters","text":"<p>Experimental feature!</p> <p>To increase usability and ensure ZARP produces reasonable results for the vast majority of RNA-Seq samples that it was built, we have consciously limited the set of tool configuration parameters that the workflow exposes through Snakemake's <code>config.yaml</code>.</p> <p>However, recognizing that power users would surely like to tweak tool parameters (e.g., to allow ZARP to be run against non-standard RNA-Seq protocols), we provide an additional, custom config file <code>rule_config.yaml</code>, which enables the user to modify (almost) any tool parameter. The values specified in this configuration override the default ones, unless they are deemed essential to the correct \"wiring\" (in which case they are considered \"immutable\" and will not be changed).</p> <p>Modifying <code>rule_config.yaml</code> thus allows users to fundamentally alter ZARP's behavior, while keeping its general wiring, all without having to make changes in the workflow definition itself.</p> <p>An example of a <code>rule_config.yaml</code> is shown below:</p> <pre><code>remove_adapters_cutadapt:\n    # Search for all the given adapter sequences repeatedly, either until no\n    # adapter match was found or until n rounds have been performed (default 1,\n    # ZARP recommends 2)\n    -n: '2'\n    # Discard processed reads that are shorter than m; note that cutadapt uses\n    # a default value of m=0, causing reads without any nucleotides remaining\n    # after processing to be retained; as \"empty reads\" will cause errors in\n    # downstream applications in ZARP, we have changed the default to m=1,\n    # meaning that only read fragments of at least 1 nt will be retained after\n    # processing. The default will be overridden by the value specified here,\n    # but for the reason stated above, we strongly recommend NOT to set m=0;\n    # cf. https://cutadapt.readthedocs.io/en/stable/guide.html#filtering-reads\n    -m: '10'\n</code></pre> <p>You can find the path to the <code>rule_config.yaml</code> file as a parameter in the standard <code>config.yaml</code> file.</p> <pre><code>rule_config: \"../input_files/rule_config.yaml\"\n</code></pre>"},{"location":"guides/parameterization/#upgrading-tool-versions","title":"Upgrading tool versions","text":"<p>As described elsewhere, ZARP's rules can be executed either with Conda or with Apptainer. The majority of the tools that we use are hosted by the Bioconda and BioContainers registries, for Conda environments and container images, respectively.</p> <p>Incase you want to upgrade one of the tools to a later version, all you need to do is to update the conda environment \"recipe\" file in <code>workflow/envs/</code> and then modify the corresponding container directive in the appropriate Snakemake workflow definition files (either in <code>workflow/Snakefile</code> or in one of the imported \"subworkflows\" in <code>workflow/rules</code>).</p> We do not recommend downgrading tool versions! <p>For security and compatibility reasons, we strongly recommend only to upgrade tool versions!</p> <p>For example, let's say you want to update <code>cutadapt</code> to the latest version:</p> <ol> <li> <p>Find the version you want to use by searching for the correspondign    tool/package name on the Anaconda website. For <code>cutadapt</code>,    the page for the specific Bioconda package is available    here. At the time of writing, the latest version    is <code>4.9</code>.</p> <p> </p> </li> <li> <p>Find the appropriate Conda enviroment recipe file in <code>workflow/envs/</code>. In    our example, it is <code>workflow/envs/cutadapt</code>, and it contains the following:</p> <pre><code>---\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - cutadapt=4.6\n...\n</code></pre> <p>Simply replace <code>4.6</code> with the <code>4.9</code> you identified in step above.</p> </li> <li> <p>Now find the corresponding BioContainers container image from the quay.io    website. Select the tags and note down one of the available versions    corresponding to <code>cutadapt 4.9</code> (note that they are available for multiple    Python versions; generally pick the most recent one you are comfortable    with). Copy the entire tag name, not just the <code>4.9</code> part!</p> <p> </p> </li> <li> <p>Finally, determine all the places where <code>cutadapt</code> is used in the workflow,    by inspecting <code>workflow/Snakefile</code> and all of the <code>.smk</code> files in    <code>workflow/rules/</code>. In the case of <code>cutadapt</code>, multiple rules use the tool,    each with the following <code>container</code> and <code>conda</code> directives.</p> <pre><code>container:\n    \"docker://quay.io/biocontainers/cutadapt:4.6--py310h4b81fae_1\"\nconda:\n    os.path.join(workflow.basedir, \"envs\", \"cutadapt.yaml\")\n</code></pre> <p>For each of these, replace the final part of the <code>container</code> directive (here: <code>4.6--py310h4b81fae_1</code>) with the tag name you copied in the step above.</p> </li> </ol> <p>That's it.</p>"},{"location":"guides/parameterization/#adding-new-tools","title":"Adding new tools","text":"<p>You may have found or developed a package that you would like to include in ZARP, so that it is always executed whenever you start a run. In this case, we highly recommend packaging the tool properly and making it available on Bioconda (see here for instructions). The advantage of this approach is that it allows you to share your tool with the broader scientific community, ensuring that it is easily accessible and installable by others. Once your package is available on Bioconda, BioContainers will automatically build a Docker image for your package (available via quay.io) - which you Apptainer will be able to use. With your Bioconda package and container image available, you can easily add additional rules to the Snakemake definition files, inside your own copy of the ZARP repository.</p> <p>Or perhaps you are convinced that every ZARP user should always run the tool or tools you have added? In that case, create a pull request against the upstream/original ZARP repository. We will evaluate your request and - if we like it and tests pass - merge it.</p> <p>Don't know how to do that? Just write us a brief email! </p>"},{"location":"guides/parameterization/#managing-tool-resources","title":"Managing tool resources","text":"<p>ZARP has been tested with many samples, and we set default parameters to ensure optimal performance of the tools across a broad range of samples (e.g., a tool can run on multiple threads). With respect to setting the maximum memory usage per rule, Snakemake provides a variable <code>mem_mb</code> in the directive <code>resources</code>. But given that the memory usage is strongly sample-dependent (sample size, souce organism), in ZARP we use dynamic memory allocation.</p> <p>We estimate the required memory based on the size of the input file using scaling factors that we have obtained empirically from the analysis of many samples. Should that initial estimate lead to a rule failing, it is re-run with an increased memory allocation. This process is repeated up to three times.</p> <p>This is what the dynamic memory allocation looks like in the workflow definition file:</p> <pre><code>resources:\n    mem_mb=lambda wildcards, attempt: 4096 * attempt,\n</code></pre> <p>In some cases the workflow might still fail. This means that you would need to manually alter the <code>mem_mb</code> used. The best solution for that is to increase the orginal memory used. In the above example, increase it from <code>4096</code> MB to a higher value, based on your expectations.</p> Globally capping resource consumption <p>Note that Snakemake provides configuration parameters to globally limit resource usage (both for memory and the number of CPUs/threads). If set, these will take precedence over the individual rule settings. This is useful if you are working on a laptop, or another machine with very limited resources. However, if your resources are lower than what the most resource-hungry tool integrated in ZARP requires as a minimum for a given run, you will not be able to complete that run.</p> <p>When you submit a workflow to an HPC cluster, additional parameters can be configured through Snakemake profiles. In the <code>profiles/</code> directory, yo can a few basic options that have proven useful for us. For example, the <code>slurm-conda</code> profile is available, and, as the name suggests, it submits jobs to a Slurm-managed HPC cluster while using Conda to manage the dependencies for each workflow rule.</p> <p>Using profiles, you can also set default resources that are applied to all jobs, unless explicitly specified. For example, if you want to set the default memory used for all rules, add or modify the following line in the profile configuration:</p> <pre><code>default-resources: mem_mb=1024\n</code></pre> Where to configure runtime limits? <p>Some parameters, including runtime limits, can only be set in your workload manager configuration. In the case of Slurm, this is in <code>slurm-config.json</code>.</p>"},{"location":"guides/usage/","title":"Execution of workflows","text":"<p>This section describes how to run ZARP, as well as the two auxiliary workflow for fetching samples from the Sequence Read Archive and populating a sparse sample table with inferred sample metadata that are also packaged within this repository for your convenience.</p> <p>Finally, we describe how you can use Docker to try to run ZARP on systems that are not natively supported (e.g., Mac OS).</p> <p>Prerequisites</p> <p>All usage examples in this section assume that you have already installed ZARP.</p>"},{"location":"guides/usage/#how-to-analyze-your-rna-seq-samples","title":"How to analyze your RNA-Seq samples?","text":"<ol> <li> <p>Assuming that your current directory is the workflow repository's root    directory, create a directory for your workflow run and traverse into it    with:</p> <pre><code>mkdir config/my_run/\ncd config/my_run/\n</code></pre> </li> <li> <p>Create an empty sample table and a workflow configuration file:</p> <pre><code>touch samples.tsv\ntouch config.yaml\n</code></pre> </li> <li> <p>Use your editor of choice to populate these files with appropriate    values. Have a look at the examples in the <code>tests/</code> directory to see what    the files should look like, specifically:</p> <ul> <li><code>samples.tsv</code></li> <li><code>config.yaml</code></li> </ul> </li> <li> <p>Create a runner script. Pick one of the following choices for either local    or cluster execution. Before execution of the respective command, you need    to remember to update the argument of the <code>--apptainer-args</code> option of a    respective profile (file: <code>profiles/{profile}/config.yaml</code>) so that    it contains a comma-separated list of all directories containing input    data files (samples and any annotation files etc) required for your run.</p> <p>Runner script for local execution:</p> <pre><code>cat &lt;&lt; \"EOF\" &gt; run.sh\n#!/bin/bash\n\nsnakemake \\\n    --profile=\"../../profiles/local-apptainer\" \\\n    --configfile=\"config.yaml\"\n\nEOF\n</code></pre> <p>OR</p> <p>Runner script for Slurm cluster execution (note that you may need to modify the arguments to <code>--jobs</code> and <code>--cores</code> in the file <code>profiles/slurm-apptainer/config.yaml</code> depending on your HPC and workload manager configuration):</p> <pre><code>cat &lt;&lt; \"EOF\" &gt; run.sh\n#!/bin/bash\nmkdir -p logs/cluster_log\nsnakemake \\\n    --profile=\"../profiles/slurm-apptainer\" \\\n    --configfile=\"config.yaml\"\nEOF\n</code></pre> </li> <li> <p>Start your workflow run:</p> <pre><code>bash run.sh\n</code></pre> </li> <li> <p>To find out more information on the expected output files, please visit the    output files description page.</p> </li> </ol> <p>Cluster configuration</p> <p>The Slurm profiles are configured for a cluster that uses the quality-of-service (QOS) keyword. If not supported by your Slurm instance, you need to remove all the lines with <code>qos</code> in <code>profiles/slurm-config.json</code>. Reach out to your systems administrator for further help adjusting the configuration to your specific HPC environment, especially when not using the Slurm workload manager.</p> Want to use Conda instead? <p>Change the argument to <code>--profile</code> from <code>local-apptainer</code> or <code>slurm-apptainer</code> to <code>local-conda</code> or <code>slurm-conda</code> instead.</p>"},{"location":"guides/usage/#how-to-fetch-sequencing-samples-from-the-sequence-read-archive","title":"How to fetch sequencing samples from the Sequence Read Archive?","text":"<p>An independent Snakemake workflow <code>workflow/rules/sra_download.smk</code> is included for the download of sequencing libraries from the Sequence Read Archive and conversion into FASTQ.</p> <p>The workflow expects the following parameters in the configuration file:</p> <ul> <li><code>samples</code>: A sample table in TSV format with column sample, containing   SRR, ERR or DRR identifiers, as in this example.</li> <li><code>outdir</code>: An output directory.</li> <li><code>samples_out</code>: The path to the output sample table containing the paths to   the corresponding outputs FASTQ files.</li> <li><code>cluster_log_dir</code>: The directory in which cluster logs are to be stored.</li> </ul> <p>You can use the following call within an activated <code>zarp</code> Conda environment to try an example run of this workflow:</p> <pre><code>snakemake \\\n  --snakefile=\"workflow/rules/sra_download.smk\" \\\n  --profile=\"profiles/local-conda\" \\\n  --config \\\n    samples=\"tests/input_files/sra_samples.tsv\" \\\n    outdir=\"results/sra_downloads\" \\\n    samples_out=\"results/sra_downloads/sra_samples.out.tsv\" \\\n    log_dir=\"logs\" \\\n    cluster_log_dir=\"logs/cluster_log\"\n</code></pre> Want to use Apptainer instead? <p>Change the argument to <code>--profile</code> from <code>local-conda</code> to <code>local-apptainer</code> to execute the workflow steps within Apptainer containers.</p> <p>After successful execution, <code>results/sra_downloads/sra_samples.out.tsv</code> should contain the following:</p> <pre><code>sample  fq1     f2\nSRR18552868     results/sra_downloads/compress/SRR18552868/SRR18552868.fastq.gz \nSRR18549672     results/sra_downloads/compress/SRR18549672/SRR18549672_1.fastq.gz       results/sra_downloads/compress/SRR18549672/SRR18549672_2.fastq.gz\nERR2248142      results/sra_downloads/compress/ERR2248142/ERR2248142.fastq.gz \n</code></pre>"},{"location":"guides/usage/#how-to-infer-sample-metadata","title":"How to infer sample metadata?","text":"<p>An independent Snakemake workflow <code>workflow/rules/htsinfer.smk</code> is available that populates the sample table required by ZARP with the sample-specific parameters <code>seqmode</code>, <code>f1_3p</code>, <code>f2_3p</code>, <code>organism</code>, <code>libtype</code> and <code>index_size</code>. Those parameters are inferred from the provided <code>fastq.gz</code> files by HTSinfer.</p> <p>Temporary directory</p> <p>The workflow uses the implicit temporary directory from Snakemake, which is called with <code>[resources.tmpdir]</code>.</p> <p>The workflow expects the following configuration parameters:</p> <ul> <li><code>samples</code>: A sample table in TSV format with columns sample (containing   sample identifiers, and fq1 and fq2 ()containing the paths to the input   FASTQ files). See an example here. If the table   contains further ZARP compatible columns (see workflow   documentation, the values specified in these by the user   are given priority over HTSinfer's results.</li> <li><code>outdir</code>: An output directory.</li> <li><code>samples_out</code>: The path to the output sample table containing the parameters   inferred by HTSinfer.</li> <li><code>records</code>: The number of sequence records to consider for inference. Set to   100'000 by default.</li> </ul> <p>You can use the following call within an activated <code>zarp</code> Conda environment to try an example run of this workflow:</p> <pre><code>cd tests/test_htsinfer_workflow\nsnakemake \\\n  --snakefile=\"../../workflow/rules/htsinfer.smk\" \\\n  --restart-times=0 \\\n  --profile=\"../../profiles/local-conda\" \\\n  --config \\\n    outdir=\"results\" \\\n    samples=\"../input_files/htsinfer_samples.tsv\" \\\n    samples_out=\"samples_htsinfer.tsv\" \\\n    log_dir=\"logs\" \\\n    cluster_log_dir=\"logs/cluster_log\" \\\n  --notemp \\\n  --keep-incomplete\n</code></pre> Want to use Apptainer instead? <p>Change the argument to <code>--profile</code> from <code>local-conda</code> to <code>local-apptainer</code> to execute the workflow steps within Apptainer containers.</p> <p>However, this call will exit with an error, as not all parameters can be inferred from the example files. The argument <code>--keep-incomplete</code> makes sure the <code>samples_htsinfer.tsv</code> file can nevertheless be inspected.</p> <p>After successful execution - if all parameters could be either inferred or were specified by the user - <code>[OUTDIR]/[SAMPLES_OUT]</code> should contain a populated table with parameters <code>seqmode</code>, <code>f1_3p</code>, <code>f2_3p</code>, <code>organism</code>, <code>libtype</code> and <code>index_size</code>.</p>"},{"location":"guides/usage/#how-to-use-docker","title":"How to use Docker?","text":"<p>Experimental feature!</p> <p>ZARP is optimised for Linux users as all packages are available via Conda or Apptainer. Execution on other systems, e.g., Mac OS X, is tricky, even more so due to the current transition from Intel to ARM processors (M series). Nevertheless, we built a Docker image that may be used to try to run ZARP in such environments (tested on Linux and Mac OS).</p> <ol> <li> <p>Install Docker following the official instructions.</p> </li> <li> <p>Pull the Docker image that contains the necessary dependencies:    <pre><code>docker pull zavolab/zarp:1.0.0-rc.1\n</code></pre></p> </li> <li> <p>Create a directory (e.g. <code>data/</code>) and store all the files required for a    run:</p> <ul> <li>The genome sequence FASTA file</li> <li>The annotation GTF file</li> <li>The FASTQ files of your experiments</li> <li>The <code>rule_config.yaml</code> for the parameters</li> <li>The <code>samples.tsv</code> containing the metadata of your samples</li> <li>The <code>config.yaml</code> file with parameters</li> </ul> <p>Below you can find an example confirguation file. Note how it points to files in the <code>data</code> directory:</p> <pre><code>---\n# Required fields\nsamples: \"data/samples_docker.tsv\"\noutput_dir: \"data/results\"\nlog_dir: \"data/logs\"\ncluster_log_dir: \"data/logs/cluster\"\nkallisto_indexes: \"data/results/kallisto_indexes\"\nsalmon_indexes: \"data/results/salmon_indexes\"\nstar_indexes: \"data/results/star_indexes\"\nalfa_indexes: \"data/results/alfa_indexes\"\n# Optional fields\nrule_config: \"data/rule_config.yaml\"\nreport_description: \"No description provided by user\"\nreport_logo: \"../../images/logo.128px.png\"\nreport_url: \"https://zavolan.biozentrum.unibas.ch/\"\nauthor_name: \"NA\"\nauthor_email: \"NA\"\n...\n</code></pre> </li> <li> <p>Execute ZARP:     <pre><code>docker run \\\n    --platform linux/x86_64 \\\n    --mount type=bind,source=$PWD/data,target=/data \\\n    zavolab/zarp:1.0.0-rc.1 \\\n    snakemake -p \\\n    --snakefile /workflow/Snakefile \\\n    --configfile data/config.yaml \\\n    --cores 4 \\\n    --use-conda \\\n    --verbose\n</code></pre></p> <p>The command runs the Docker container <code>zavolab/zarp:1.0.0-rc.1</code> that we have pulled. It executes it, just as it would be done on a Linux platform (<code>--platform linux/x86_64</code>). We use the <code>--mount</code> option to bind the local <code>data/</code> directory that contains the input files to the <code>/data/</code> directory inside the container. The workflow definition file is stored at <code>/workflow/Snakefile</code> inside the container.</p> <p>Once ZARP has completed, the results will be available in the <code>data/results/</code> directory.</p> </li> </ol>"}]}